{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121afea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f4bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary():\n",
    "    \"\"\"\n",
    "    This class(Vocabulary), builds a character-level vocabulary for a given list of words.\n",
    "    It initializes the vocabulary with four special tokens (PAD, SOW, EOW, and UNK) and creates\n",
    "    two dictionaries (stoi and itos) to map characters to integers and vice versa.\n",
    "    Tokenizer: Tokenizes a given text into individual characters.\n",
    "    build_vocabulary(): Takes a list of words and adds each unique character \n",
    "    to the vocabulary, along with a unique integer ID.\n",
    "    numericalize(): Converts a given text into a list of integers, where each \n",
    "    integer corresponds to the ID of a character in the vocabulary.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.itos = {0:\"<PAD>\",1:\"<SOW>\",2:\"<EOW>\",3:\"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\":0,\"<SOW>\":1,\"<EOW>\":2,\"<UNK>\":3}\n",
    "        #self.freq_threshold = freq_threshold\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenizer(text):\n",
    "        return [*text]\n",
    "    \n",
    "    def build_vocabulary(self, word_list):\n",
    "        char_list = []\n",
    "        idx = 4\n",
    "        \n",
    "        for word in word_list:\n",
    "            for char in self.tokenizer(word):\n",
    "                if char not in char_list:\n",
    "                    char_list.append(char)\n",
    "                    self.stoi[char] = idx\n",
    "                    self.itos[idx] = char\n",
    "                    idx+=1\n",
    "                    \n",
    "                    \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer(text)\n",
    "        \n",
    "        return [self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text]\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e31bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class aksharantar(Dataset):\n",
    "    \"\"\"\n",
    "    This class used to process text data for a machine translation task.\n",
    "    root_dir: the root directory where the data is stored\n",
    "    out_lang: the target language for translation \n",
    "    dataset_type: either \"train\", \"test\", or \"val\" indicating which dataset is being used.\n",
    "    After loadinf data __init__() builds the vocabulary for each language by adding all unique characters in \n",
    "    the language's text data to the corresponding Vocabulary object.\n",
    "    The __getitem__() method takes an index and returns the numericalized form of the corresponding input \n",
    "    and output sentences.\n",
    "    It tokenizes each sentence into characters and adds special start-of-word (<SOW>) and end-of-word (<EOW>) \n",
    "    tokens to the beginning and end of the numericalized output sentence.\n",
    "    Finally, it returns PyTorch tensors of the numericalized input and output sentences.\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, root_dir, out_lang, dataset_type): \n",
    "        \n",
    "        # Read file\n",
    "        self.file_name = out_lang + \"_\" + dataset_type + \".csv\"\n",
    "        self.file_dir = os.path.join(root_dir, out_lang, self.file_name)\n",
    "        self.df = pd.read_csv(self.file_dir, names = [\"latin\", \"hindi\"])\n",
    "        \n",
    "        # Get columns of input and output language\n",
    "        self.latin = self.df[\"latin\"]\n",
    "        self.hindi = self.df[\"hindi\"]\n",
    "        \n",
    "        # Initialize vocabulary and build vocab\n",
    "        self.vocab_eng = Vocabulary()\n",
    "        self.vocab_eng.build_vocabulary(self.latin.tolist())\n",
    "        \n",
    "        # Initialize vocabulary and build vocab\n",
    "        self.vocab_hin = Vocabulary()\n",
    "        self.vocab_hin.build_vocabulary(self.hindi.tolist())\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        latin = self.latin[index]\n",
    "        hindi = self.hindi[index]\n",
    "        \n",
    "        numericalized_hindi = [self.vocab_hin.stoi[\"<SOW>\"]]\n",
    "        numericalized_hindi += self.vocab_hin.numericalize(hindi)\n",
    "        numericalized_hindi.append(self.vocab_hin.stoi[\"<EOW>\"])\n",
    "        \n",
    "        numericalized_latin = [self.vocab_eng.stoi[\"<SOW>\"]]\n",
    "        numericalized_latin += self.vocab_eng.numericalize(latin)\n",
    "        numericalized_latin.append(self.vocab_eng.stoi[\"<EOW>\"])\n",
    "        \n",
    "        return torch.tensor(numericalized_latin), torch.tensor(numericalized_hindi) \n",
    "               \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22204b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCollate:\n",
    "    \"\"\"\n",
    "    This class is used to collate the data items into batches for DataLoader. \n",
    "    It takes two arguments, pad_idx_eng and pad_idx_hin, which are the index of the <PAD> token\n",
    "    in the English and Hindi vocabularies respectively.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, pad_idx_eng, pad_idx_hin):\n",
    "        self.pad_idx_eng = pad_idx_eng\n",
    "        self.pad_idx_hin = pad_idx_hin\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        inputs = [item[0] for item in batch]\n",
    "        inputs = pad_sequence(inputs, batch_first=False, padding_value=self.pad_idx_eng)\n",
    "        \n",
    "        targets = [item[1] for item in batch]\n",
    "        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx_hin)\n",
    "        \n",
    "        return inputs, targets\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb69e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(root_dir, out_lang, dataset_type, batch_size, pin_memory=True ):\n",
    "    \"\"\"\n",
    "    This class returns a PyTorch DataLoader object and a custom dataset object. \n",
    "    The DataLoader object loads the data in batches.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = aksharantar(root_dir, out_lang, dataset_type)\n",
    "    \n",
    "    pad_idx_eng = dataset.vocab_eng.stoi[\"<PAD>\"]\n",
    "    pad_idx_hin = dataset.vocab_hin.stoi[\"<PAD>\"]\n",
    "    \n",
    "    loader = DataLoader(dataset=dataset,batch_size=batch_size,\n",
    "                       pin_memory=pin_memory,\n",
    "                       collate_fn=MyCollate(pad_idx_eng=pad_idx_eng, pad_idx_hin=pad_idx_hin),\n",
    "                       shuffle=True)\n",
    "    return loader, dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d256b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This code defines an Encoder class for a sequence-to-sequence model.\n",
    "    The class takes in an input size, embedding size, hidden size, \n",
    "    number of layers, dropout rate, cell type (GRU, LSTM, or RNN), \n",
    "    and whether the network is bidirectional. The forward method takes in \n",
    "    an input tensor x, applies dropout to its embedded representation, and \n",
    "    passes it through a GRU, LSTM, or RNN layer depending on the cell type specified. \n",
    "    The final hidden states of the layer(s) are returned.\n",
    "    \n",
    "    \"\"\"\n",
    "    #input_size represents the dimensionality of the \n",
    "    #encoder's input space, indicating the number of possible input tokens or\n",
    "    #categories that the coder can generate.\n",
    "    \n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p, cell_type, bi_directional):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.cell_type = cell_type\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        \n",
    "        if bi_directional:\n",
    "            self.fc_hidden = nn.Linear(2*hidden_size, hidden_size)\n",
    "        else:\n",
    "            self.fc_hidden = nn.Linear(hidden_size, hidden_size) \n",
    "            \n",
    "        if bi_directional:\n",
    "            self.fc_cell = nn.Linear(2*hidden_size, hidden_size)\n",
    "        else:\n",
    "            self.fc_cell = nn.Linear(hidden_size, hidden_size) \n",
    "            \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, num_layers, dropout=p, bidirectional=bi_directional)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p,bidirectional=bi_directional)\n",
    "        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, dropout=p,bidirectional=bi_directional)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x, shape=(seq_length, N)\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape = (seq_length, N,embedding_size )\n",
    "        \n",
    "        if self.cell_type == 'gru':\n",
    "            encoder_states, hidden = self.gru(embedding)\n",
    "            hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2)\n",
    "            return encoder_states, hidden\n",
    "        \n",
    "        if self.cell_type == 'lstm':\n",
    "            encoder_states, (hidden, cell) = self.lstm(embedding)\n",
    "            hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\n",
    "            cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\n",
    "            return encoder_states, hidden, cell\n",
    "        \n",
    "        if self.cell_type == 'rnn':\n",
    "            encoder_states, hidden = self.rnn(embedding)\n",
    "            hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2)\n",
    "            return encoder_states, hidden\n",
    "        \n",
    "          \n",
    "    # This method is called at the beginning of each new input sequence\n",
    "    # to reset the hidden state.\n",
    "#     def initHidden(self):\n",
    "#         return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cf1ce3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This code defines the Decoder class, which is responsible for decoding the encoded input sequence\n",
    "    and generating the target sequence. \n",
    "    The method first unsqueezes x to add a batch dimension and then applies dropout to the embedding layer. \n",
    "    It then passes the embedded input sequence through the decoder's RNN layer, \n",
    "    which can be either GRU, LSTM, or RNN.\n",
    "    Then passes the output through a linear layer to get the predictions, which are returned \n",
    "    along with the hidden and cell states.\n",
    "    Finally, the method squeezes the predictions tensor to remove the batch dimension before returning \n",
    "    the predictions and hidden/cell states.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers,\n",
    "                 p, cell_type, bi_directional ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.cell_type = cell_type\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.energy = nn.Linear(hidden_size * 3, 1)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2 + embedding_size, hidden_size, num_layers, dropout=p,bidirectional=bi_directional )\n",
    "        self.lstm = nn.LSTM(hidden_size * 2 + embedding_size, hidden_size,num_layers, dropout=p, bidirectional=bi_directional)\n",
    "        self.rnn = nn.RNN(hidden_size * 2 + embedding_size, hidden_size,num_layers, dropout=p, bidirectional=bi_directional)\n",
    "        if bi_directional:\n",
    "            self.fc = nn.Linear(2*hidden_size, output_size)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, output_size)        \n",
    "        \n",
    "    def forward(self, x, encoder_states, hidden, cell):\n",
    "        # x, shape=(N) but we want (1, N)\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape = (1, N,embedding_size )\n",
    "        \n",
    "        sequence_length = encoder_states.shape[0]\n",
    "        h_reshaped = hidden.repeat(sequence_length, 1, 1)\n",
    "        \n",
    "        energy = self.relu(self.energy(torch.cat((h_reshaped, encoder_states), dim=2)))\n",
    "        \n",
    "        attention = self.softmax(energy)\n",
    "        # attention: (seq_length, N, 1)\n",
    "        \n",
    "        context_vector = torch.einsum(\"snk,snl->knl\", attention, encoder_states)\n",
    "\n",
    "        rnn_input = torch.cat((context_vector, embedding), dim=2)\n",
    "        # rnn_input: (1, N, hidden_size*2 + embedding_size)\n",
    "        \n",
    "        if self.cell_type == 'gru':\n",
    "            outputs, hidden = self.gru(rnn_input, hidden)\n",
    "            #shape of output (1,N,hidden_size)\n",
    "            \n",
    "        if self.cell_type == 'lstm':\n",
    "            outputs, (hidden, cell) = self.lstm(rnn_input, (hidden, cell))\n",
    "            \n",
    "        if self.cell_type == 'rnn':\n",
    "            outputs, hidden = self.rnn(rnn_input, hidden)\n",
    "            \n",
    "        predictions = self.fc(outputs).squeeze(0)\n",
    "        # shape of predictions = (1, N, length_of_vocabs)\n",
    "        \n",
    "        \n",
    "        if self.cell_type == 'lstm':\n",
    "            return predictions, hidden, cell\n",
    "        else:\n",
    "            return predictions, hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb6b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    This class have functions which takes words as input and target words to find the \n",
    "    predictions using the model build in the forward function.\n",
    "    This function uses the encoder and decoder formed earlier.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, cell_type):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.cell_type = cell_type\n",
    "        \n",
    "    def forward(self, word_input, word_target, teacher_force_ratio=0.5):\n",
    "        \n",
    "        batch_size = word_input.shape[1]\n",
    "        target_length = word_target.shape[0]\n",
    "        \n",
    "        outputs = torch.zeros(target_length, batch_size, len(train_data.vocab_hin)).to(device)\n",
    "        \n",
    "        if self.cell_type == 'lstm':\n",
    "            encoder_states, hidden, cell = self.encoder(word_input)\n",
    "        else:\n",
    "            encoder_states, hidden = self.encoder(word_input)\n",
    "            \n",
    "        # grab start token\n",
    "        x= word_target[0]\n",
    "        \n",
    "        for t in range(1, target_length):\n",
    "            if self.cell_type == \"lstm\":\n",
    "                output, hidden, cell = self.decoder(x, encoder_states, hidden, cell)\n",
    "            else:\n",
    "                output, hidden = self.decoder(x, encoder_states, hidden, 0)\n",
    "                \n",
    "            outputs[t] = output\n",
    "            \n",
    "            best_pred = output.argmax(1)\n",
    "            \n",
    "            x = word_target[t] if random.random() < teacher_force_ratio else best_pred\n",
    "            \n",
    "        return outputs\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
